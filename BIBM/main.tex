\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfigure}
\def\UrlBreaks{\do\A\do\B\do\C\do\D\do\E\do\F\do\G\do\H\do\I\do\J
\do\K\do\L\do\M\do\N\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V
\do\W\do\X\do\Y\do\Z\do\[\do\\\do\]\do\^\do\_\do\`\do\a\do\b
\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n
\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z
\do\.\do\@\do\\\do\/\do\!\do\_\do\|\do\;\do\>\do\]\do\)\do\,
\do\?\do\'\do+\do\=\do\#} 
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{High-Performance scRNA-seq Data Processing with Low-Redundancy Disk Access}

\author{\IEEEauthorblockN{Yu Liu}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
liuyu123@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Mingxuan Gao}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
mingxuangao@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Lixuan Tan}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
tanlix@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Hongjin Liu}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
liuhongjin@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Yating Lin}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
linyating@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Wenxian Yang}
\IEEEauthorblockA{\textit{Aginome Scientific} \\
Xiamen, China \\
wx@aginome.com}
\and
\IEEEauthorblockN{Rongshan Yu*}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
rsyu@xmu.edu.cn}
}

\maketitle

\begin{abstract}
High-throughput single-cell RNA sequencing (scRNA-seq) data processing pipelines integrate multiple modules to transform raw scRNA-seq data to gene expression matrices, including barcode processing, sequence quality control, genome alignment and transcript quantification.
With the rapid growth in data volume, the speed of scRNA-seq data processing pipeline has become a major bottleneck to large-scale scRNA-seq studies. 
We present scSpark, a cloud computing based scRNA-seq data processing pipeline. 
By leveraging Apache Spark's in-memory computing capability, scSpark significantly improves the processing speed of scRNA-seq data, and achieves 5\-20 times faster than the state-of-the-art processing pipelines under the same CPU core consumption.
In addition, thanks to Spark's inherent scalability in a cloud computing environment, scSpark can further reduce the processing time for a typical scRNA-seq dataset (e.g., 640 million reads) from hours to minutes when multiple computer nodes (e.g., 16) are used.  
Biological evaluation also confirmed that the results generated by scSpark are highly consistent with existing scRNA-seq data processing pipelines.
\end{abstract}

\begin{IEEEkeywords}
scRNA-seq data processing, Apache Spark, cloud computing
\end{IEEEkeywords}

\section{Introduction}
\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{fig1.pdf}
	\caption{Using JNI and RDD to integrate the STAR aligner in scSpark.} \label{fig1}
\end{figure*}
Single cell is the fundamental unit of a living organism.
Historically, RNA-seq has been widely used to study gene expression patterns in biological samples.
However, the resolution of bulk RNA-seq could only reach the average level of cell populations. 
With the development of single-cell sequencing technologies, scRNA-seq now allows transcript profiling of tens of thousands of cells simultaneously in a single experiment, and has emerged as a powerful tool to identify and characterize cell types in complex and heterogeneous biological samples~\cite{Zhang2019ComparativeAO}.

Generally, a fully-functioned scRNA-seq data processing pipeline typically implements multiple modules including the unique molecular identifier (UMI) barcode~\cite{Smith2017UMItools} processing, sequence quality control (QC)~\cite{schmieder2011quality}, genome alignment~\cite{Dobin2013STAR,Kim2015HISAT} and transcript quantification~\cite{Parekh2018zUMIs} to convert raw scRNA-seq data into a gene expression matrix for further downstream analysis. 
To enable efficient scRNA-seq data processing, various pipelines have been developed, among which the most influential studies probably include CellRanger~\cite{Zheng2017Massively}, UMI-tools~\cite{Smith2017UMItools}, and STARsolo~\cite{Blibaum2019STARsolo}, etc. 
CellRanger is a highly integrated data processing software tool tailored by 10X Genomics for scRNA-seq data analysis.
It is suitable for processing large datasets on high performance workstations~\cite{Gao2020Comparison}. 
UMI-tools is a comprehensive scRNA-seq data processing suite with the directional barcode collapse algorithm integrated that considerably promotes the transcript quantification accuracy.
STARsolo is a recently developed pipeline extended from the genome aligner STAR to adapt to single cell applications, so that users are enabled to align scRNA-seq reads conveniently to reference genomes using STAR without assistance of any extra tools.

In the big biological data era, 
the computational demand originated from the large volume of scRNA-seq data of increasing numbers of single cell studies is becoming tremendous and surpassed the capabilities of these traditional bioinformatics tools. 
The cloud computing environment is a distributed system with extremely scalable computation capabilities, and allows users to run applications and services on a distributed network using a virtualized system. 
Recently, big data frameworks such as Apache Hadoop (\url{https://hadoop.apache.org}) and Apache Spark (\url{https://spark.apache.org}) have been used to speed up the data processing for next generation sequencing (NGS) data. 
SparkBWA~\cite{Abun2016SparkBWA} exploits the capabilities of Spark to boost the performance of a most widely used NGS data aligner, the Burrows-Wheeler Aligner (BWA). 
GPF~\cite{Li2018Highperformance} is a fast in-memory computing framework designed using the Spark framework for implementing NGS data processing pipelines in cloud computing environment. 
Following the success of cloud-based implementations of NGS data processing pipelines, Falco~\cite{Yang2017Falco} concatenates the genome aligner and transcript quantification software tools using Spark for scRNA-seq data preprocessing.
Although Falco improves the performance of scRNA-seq data processing when a distributed computing environment is available, it does not support UMI barcode processing. Hence, it is incompatible with the widely-used high-throughput scRNA-seq protocols such as 10X Genomics, Drop-seq and Microwell-seq. In addition, it does not leverage the in-memory computing capability of Spark to reduce the disk read and write operations of intermediate data processing steps. Therefore, further performance speed-up could be expected. 

To meet the growing computational demand of processing large-scale scRNA-seq data, we present scSpark, an in-cloud and in-memory computing scRNA-seq data processing pipeline with high efficiency and scalability. 
More specifically, by implementing and integrating multiple procedures of a standard scRNA-seq data processing pipeline with the aid of in-memory computing schema of Spark, scSpark eliminates the need for laborious disk read/write operations associated with traditional bioinformatics tools. 
As a result, scSpark is able to improve the speed for scRNA-seq data processing by more than 10 folds compared with other state-of-the-art software tools under the same CPU core consumption. In addition, by harnessing the merit of the parallel computing capability of Spark engine, scSpark further enables users to distribute their scRNA-seq data processing workloads to multiple computational nodes, thus dramatically increases their processing throughput of scRNA-seq data for large-scale studies. 
scSpark is freely available at \url{https://github.com/xmuyulab/spark-scRNASeq-Analysis.git}.

\section{Method}
\subsection{Overview}
Apache Spark~\cite{zaharia2010spark} is a high performance, in-memory and distributed computation engine for large-scale data processing. 
In Spark, Resilient Distributed Dataset (RDD)~\cite{Zaharia2012Resilient} is designed as a read-only data container that can be divided into logical partitions across a cluster of computational nodes for parallel computing. 
By using RDDs, Apache Spark allows to cache data in memory, hence reduces time for redundant disk I/O.
In this study, we developed scSpark based on Spark framework to leverage its two major features for ultra-fast scRNA-seq data processing, namely distributed and in-memory computing. 
Moreover, the workflow of scSpark follows closely with that of UMI-tools. Briefly, our scRNA-seq data processing pipeline consists of three major steps, data loading with whitelist control, alignment of extracted reads to a reference genome, and transcript quantification. 

\subsection{Data loading with whitelist control}
scSpark loads scRNA-seq reads from sequencing result files simlilarly to other software tools. To construct RDD that contains reads for subsequent parallel in-memory processing, e.g., alignment, the following procedures are used (Fig.~\ref{fig1}).

For most data yielded from high-throughtput scRNA-seq platforms, two FASTQ~\cite{cock2010sanger} files are generated to save the paired-end sequencing results where the first FASTQ file contains information of cell barcodes and UMIs, and the second one contains transcript sequences.
First, scSpark loads scRNA-seq reads from disks in parallel into two RDDs, which are FASTQ R1 RDD for the first read and FASTQ R2 RDD for the second read respectively by using \textit{newAPIHadoopFile} Spark function and reimplementing Hadoop-BAM~\cite{hadoopBAM}.

Subsequently, scSpark extracts cell barcodes and UMIs from FASTQ R1 RDD as a key-value structure where the key is cell barcode and the value is UMI. Then the highest-occurrent cell barcodes are identified with a user-defined threshold to create the Whitelist RDDs~\cite{guo2018bioinformatics}, so that the unwanted reads can be discarded with \textit{join} function. \textit{zipWithIndex} function is used to index both read-pair FASTQ RDD to preserve the corresponding relationship of the cell barcode, UMI and transcript sequence.

\subsection{Parallel genome alignment}

As in CellRanger, UMI-tools and STARsolo, we also adopt STAR as the aligner in scSpark.
The conventional STAR alignment procedure loads raw reads from FASTQ files and writes alignment results to Sequence Aligment/Map (SAM) or the compressed binary version of a SAM (BAM)~\cite{li2009sequence} file, requiring extensive disk access which consumes a significant amount of time.

Instead, in scSpark, we use the Java Native Interface (JNI)~(\cite{kim2012benchmarking}) to directly feed FASTQ RDD to STAR, and further transfer the results output from STAR to the SAM RDD after alignment (Fig.~\ref{fig1}.a). 
In such way, the alignment process is performed in an in-memory computing manner without any redundant disk access causes from temporyary result. 

To balance the workloads across different nodes for parallel processing, we use the Spark function \textit{repartition} before alignment to split FASTQ RDDs into \textit{N} subsets where \textit{N} denotes the number of nodes available in the cluster.
After data partitioning and shuffling, the STAR alignment is performed in parallel on each computational nodes, directly resulting in a SAM RDD which is used as an input to the next module.

\subsection{Transcript quantification}
For transcript quantification, we first use the \textit{textFile} function in Spark to load the gene transfer format (GTF)~\cite{breese2013ngsutils} file which contains the gene annotation information into memory as GTF RDDs (Fig.~\ref{fig1}.b). 
After alignment, both SAM RDDs and GTF RDDs are grouped by chromosome into Grouped SAM RDDs and Grouped GTF RDDs, respectively, for parallel processing by the Spark function \textit{groupBy}. 
Then, the Grouped GTF RDD and Grouped SAM RDD are combined by chromosome to generate the Chromosome Partition RDD by using the \textit{join} function.
At this stage, the gene names are assigned to each read according to its alignment coordinates by a user-defined \textit{flatMap} function. 
Subsequently, we reimplemented the directional algorithm designed by UMI-tools for transcript quantification in parallel. The quantification results are saved in Counted RDD.
Finally, the resulting expression matrix are generated on each node and wrote to disk or distributed file system by using default Spark's action function \textit{saveAsTextfile} for further downstream analysis. 

\subsection{Benchmarking experiment configurations}
The performance of scSpark was benchmarked with CellRanger, UMI-tools and STARsolo in terms of processing speed. 
We used Apache Spark (version 2.1.0) as scSpark's in-memory computing environment.
The computational cluster used consists of 8 workstations with 32 \textit{Intel Xeon (Cascade Lake) Platinum 8269CY} CPU cores at 2.5 GHz for each node. 
Note that for experiments with no more than 64 cores, each workstation was logically split into two Spark executors such that each executor contains 16 CPU cores and 64 GB of DRAM. 
For experiments using 128 to 256 cores, we use the 8 workstations directly as 8 nodes with a maximum utilization of 32 CPU cores each. 
For other conventional pipelines under evaluation, we used a workstation with 64 \textit{Intel(R) Xeon(R) CPU E5-2683 v4} CPU cores at 2.1 GHz and 256 GB DRAM. Pre-tests were performed to ensure sufficient memory for each pipeline.

Three public scRNA-seq datasets containing peripheral blood mononuclear cells (PBMCs) from three different species, namely human, rat and monkey, generated by 10X Genomics platforms were used in our experiments. 
In total, there were approximately 640 millions, 289 millions and 262 millions reads in the raw data of the PBMC\_human, PBMC\_rat and PBMC\_monkey datasets, respectively. 
In all the experiments, we run all the four pipelines with exactly the same cell number and barcode pattern arguments, and the number of CPU cores used was configured using the \textit{runThreadN} parameter for STAR and STARsolo, the \textit{localcores} parameter for CellRanger and the \textit{SPARK\_EXECUTOR\_CORES} parameter for scSpark.

%Firstly, we compared scSpark's performance on a cluster with 64 (16$\times$4) CPU cores with the other three pipelines' performance on a workstation on 64 CPU cores on PBMC\_human.
%To test improvement in each step performance of scSpark, we meatured process time of UMI-tools and scSpark each substep to prove scSpark get improve in any single substep on 64 CPU cores.

%Then we tested performance of each pipelines on PBMC\_human when CPU cores number is up to 64.
%Due to limit of single workstation CPU cores, we tested STARsolo and CellRanger's speed-up from 32 CPU cores to 64 CPU cores.
%And due to limit of memory capacity per node, we tested scSpark's speed-up in 32 (2$\times$16) CPU cores, 48 (3$\times$16) CPU cores to 64 (4$\times$16) CPU cores.
%Moreover we tested performance of scSpark on PBMC\_human when CPU cores number is between 128 and 256.

%Particularly, we tested the mapping speed of STAR on PBMC\_human dataset when CPU cores number is up to 64.
%Furthermore, we tested the mapping speed of scSpark on three datasets when CPU cores number is between 128 and 256.

%Also we splited PBMC\_human dataset into 80 millions, 160 millions and 320 millions reads.
%We processed PBMC\_human and three sub datasets on scSpark with different CPU cores number condition.
%After that we compare each dataset's performance to evaluated whether FASTQ data volume will influence scSpark's performance.

%Our scSpark is developed based on UMI-tools and UMI-tools accuracy was fully verified. 
%This section we used the gene expression matrix that was obtained by scSpark and UMI-tools under the same dataset to perform downstream analysis of scRNA-seq data. 
%And then we compared two tools transcript analysis's result and cell cluster analysis's result to verify the correlation between scSpark and UMI-tools. 
%Under PBMC\_human dataset, we used scSpark and UMI-tools to get gene express matrix, compared two tools' result, and computed their correlation.

\section{Results}

\begin{figure*}
	\centering
	\subfigure[Barcode extraction and sequence QC]{\includegraphics[width=0.32\textwidth]{fig2a.pdf}}
	\subfigure[Alignment]{\includegraphics[width=0.32\textwidth]{fig2b.pdf}}
	\subfigure[Transcript counting]{\includegraphics[width=0.32\textwidth]{fig2c.pdf}}
	\caption{Comparison of processing time between UMI-tools and scSpark, breakdown by steps.}
	\label{fig2}
\end{figure*}

\subsection{Efficiency evaluation}

\begin{table}
	\centering
	\caption{Comparison of processing time (seconds) of the four pipelines.}\label{tab1}
	\begin{tabular}{l | l | l | l }
		\hline
		 & PBMC\_human & PBMC\_rat & PBMC\_monkey \\ 
		\hline
		UMI-tools & 7284 & 7259 & 7809 \\
		CellRanger & 2225 & 1999 & 1891 \\
		STARsolo & 1987 & 1854 & 2357 \\
		\textbf{scSpark} & \textbf{355} & \textbf{326} & \textbf{391} \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{fig3.pdf}
	\caption{Comparison in scalability of CellRanger, STARsolo, and scSpark. The plot at the top-right corner of this figure shows the performance of scSpark with the total number of cores ranging from 128 to 256. Note that experiments for scSpark using 32 to 64 cores were conducted using 4 nodes with 64 CPU cores each, while those using 128 to 256 cores were conducted using 8 nodes with 32 CPU cores each.}
	\label{fig3}
\end{figure}
\begin{figure*}
	\centering
	\subfigure[Alignment speed comparison between STAR and scSpark]{\includegraphics[width=0.45\textwidth]{fig4a.pdf}}
	\subfigure[Alignment speed of scSpark with regard to the number of CPU cores.]{\includegraphics[width=0.45\textwidth]{fig4b.pdf}}
	\caption{Evaluation of the alignment speed with regard to the number of CPU cores.}
	\label{fig4}
\end{figure*}
\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{fig5.pdf}
	\caption{Comparison of the speed-up of scSpark with regard to data volume.} \label{fig5}
\end{figure}

To illustrate the efficiency improvement brought by scSpark, we first compared the total time consumption of scSpark with that of UMI-tools, STARsolo and CellRanger, when the same number of CPU cores are used. Specifically, scSpark was run in a distributed way on four computational nodes with 16 CPU cores in each node, while a 64-CPU workstation was used to run the other three pipelines separately. 
Results show that scSpark achieved substantially higher processing speed, which was nearly 20-fold faster than UMI-tools and about 5-fold faster than CellRanger and STARsolo (Table~\ref{tab1}) for all the three datasets tested.  

We also recorded the processing time for the three individual modules of the pipelines, i.e., barcode extraction and sequence QC, genome alignment, and transcript quantification, to have a more detailed understanding of the performance gain. 
As scSpark is built based on UMI-tools, the breakdown comparison was only between UMI-tools and scSpark to demonstrate the performance gain brought by distributed and in-memory computing. 
Compared with UMI-tools, scSpark had significantly shorter processing time in all three steps (Fig.~\ref{fig2}).
It is noteworthy that scSpark achieved a processsing speed of approximately 50-fold faster than UMI-tools for the barcode extraction and sequence QC step, which turned out to be the dominant factor of the performance gain brought by scSpark. 

These results show that by distributed loading and processing of FASTQ reads from disk, and the avoidance of writing alignment results back to disk, scSpark can dramatically reduce the time consumption in scRNA-seq data processing. 

\subsection{Scalability evaluation}
With the rapid growth of the data volume in scRNA-seq studies, it is desired that data processing software tools should make full usage of available computational resources to maximize the efficiency in large-scale data processing. To this end, we evaluated the scalability of scSpark and compared with the other three pipelines in terms of data processing speed acceleration with regard to the number of processing threads or CPU cores and the data volume.

\subsubsection{Overall scalability to the number of CPU cores}
First, we tested whether the data processing speed of these pipelines could increase when more threads or CPU cores were used. Herein, we used the speed-up metric, which was calculated as the ratio of the baseline time consumption when 32 CPU cores were available to the time consumption when other number of CPU cores were used, to reveal the cability in computational resouce utilization of various pipelines. 
We used PBMC\_human for this test as this dataset has the largest number of reads among the three testing datasets, which helps to minimize the impact of the overhead for multi-threading on the evaluation results. 
Our results (Fig~\ref{fig3}) show that CellRanger almost remained constant running speed with a speed-up factor of around 1 for all testing points, which indicates its poor scalability to the number of cores. 
STARsolo achieved a relatively high speed-up rate from 32 to 48 cores before a significant drop when more than 48 cores were used. When using 64 cores, the speed of STARsolo is almost the same as using 32 cores, with a speed-up factor of 1.076. 
The decrease in its speed-up rate is probably due to the overhead in creating threads and joining results on a single workstation which increases with the number of threads used. 
Compared with these two pipelines, scSpark remained a nearly linear acceleration of data processing in a wide range of number CPU cores from 32 to 256, demonstrating its superior scalability. 

In its current implementation, UMI-tools only supports single-thread mode and does not provide any argument to set the number of threads or CPU cores. Only its alignment module that adopts the STAR aligner could take advantage of more CPU cores. 
In our pre-test, we run UMI-tools on different workstations with 16, 32, 64 CPU cores separately and found that the difference in processing time is negligible. 
Therefore UMI-tools has limited scalability and is excluded from this evaluation. 

\subsubsection{Scalability of the alignment step to the number of CPU cores}
As all the four pipelines adopt STAR for sequence alignment, we further evaluated the scalability of the alignment step. 
In particular, we measured the alignment speed and compared betwen STAR and scSpark when different number of threads or CPU cores were used. 
Fig.~\ref{fig4} shows the number of reads being aligned by STAR and scSpark with different numbers of CPU cores. 
STAR consists of \textit{load} and \textit{mapping} and for this evaluation we only measure the time consumed for the \textit{mapping} module. Therefore both scSpark and STAR extracted reads data from memory and the time for loading FASTQ files from harddisks was not included in the comparison. However, scSpark outputs alignment results to memory (SAM RDD) while STAR outputs alignment results to harddisk as a SAM file.  
For scSpark, the number of reads processed per hour increases linearly with the number of CPU cores, showing that scSpark took advantages of distributed computing and in-memory computing. 
For STAR, the output of alignment results to harddisk has probably created a bottleneck such that the processing speed hardly increased when more than 32 CPU cores were used. 
%The aligner of all four pipelines is based on STAR, and in pretest shows the scalability of UMI-tools most comes from align step.
%So we particulaly measured mapping speed of STAR and scSpark.

%As Fig~\ref{fig4}.a shown, we found in align step, scSpark can get much higher mapping speed than STAR in any same CPU cores number condition.
%This shows scSpark can take advantage of scSpark reimplement the method to storage the temporary result of pipeline in align step.
%Furthermore, we found the mapping speed of scSpark can get nearly linear parallel efficiency with CPU cores number increase.
%Although the mapping speed of STAR can get improve when CPU core number between 16 and 32, the improve of the mapping speed of STAR will converage when CPU cores number bigger than 32.

Further, as shown in Fig.~\ref{fig4}.b, the alignment speed of scSpark increased linearly for all testing points when the number of CPU cores used ranges from 128 to 256. This accounts to the full utilization of 32 CPU cores for each of the 8 computational nodes.  
%And as Fig~\ref{fig4}.b shown, the mapping speed of scSpark can get linear parallel efficiency even CPU cores number scale out to 256.
%This shows the scalability of scSpark is much more efficiency than STAR in align step.

\subsubsection{The scalability of scSpark to data volume}
\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{fig6.pdf}
	\caption{correlation of scSpark and UMI-tools.} \label{fig6}
\end{figure}
\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{fig7.pdf}
	\caption{tSNE picture based on scSpark and UMI-tools' gene expression matrix.} \label{fig7}
\end{figure*}

We further evaluated the performance of scSpark in speed-up rate with regard to different amount of data processed.  
We tested scSpark with datasets of 80, 160, 320 and 640 millions of reads, all extracted from the PBMC\_human dataset, on 8 computational nodes with a maximum of 32 CPU cores each for scSpark. 
As Fig.~\ref{fig5} shows, the speed-up rate increases with the number of CPU cores for all tested data volume. 
However, the speed-up rate with smaller size of data is generally lower that that with larger size of data. 
In scSpark, each computational node needs to load the reference genome in FASTA format from harddisk before alignment with STAR, resulting in an overhead that cannot be ignored when the number of reads processed is small. 
This could possibly explain that the speed-up rate increased slower with small data volume when the number of CPU cores is high.  
For large data volume, e.g., 640 million reads, the speed-up rate increased linearly even when all the CPU cores were engaged.  
This result demonstrates that scSpark performed well to maximize the utilization of available computational resources. It could be further optimized for small data volume by re-implementing the loading of the reference genome from FASTA files into a FASTQ RDD before alignment, which is not implemented in our current version as we designed scSpark mainly for large-scale scRNA-seq studies. 

%In experiment, we also found the performance of scSpark influenced by the volume of FASTQ data.
%We think if the scalability of scSpark can improve with FASTQ

%As Fig~\ref{fig5} shown, when FASTQ dataset contains 80 millions reads, the speed-up of scSpark is much less than FASTQ dataset which contains 640 millions in CPU cores number between 128 and 256 condition.
%Moreover, the speed-up of scSpark will converage when dataset contains 80 millions reads or 160 millions reads and CPU cores number bigger than 192.
%Contrast with small dataset, the speed-up of scSpark can get increase when dataset contains 320 millions reads or 640 millions reads and CPU cores number bigger than 192.
%So scSpark can scala-out more efficiency when scSpark processes larger dataset.

%Our scSpark does not reimplement STAR's genome load method in align step which lack scalability in align step may be charged with some responsibility of scalability of scSpark scalability influenced by data volume.
%We found both scSpark and STAR lack scalability in this substep because genome load method of STAR does not influencd by CPU cores number nor input dataset volume.
%The process time of this substep occupy larger part of dataset which contains less reads causes the scalability of whole substep is weak. 

\subsection{Reliability of the results produced by scSpark} 
We further evaluated the accuracy of the expression matrices generated by scSpark.
First, the total UMI count for each cell in PBMC\_human dataset produced by scSpark was highly consistent with that by UMI-tools, with a Pearson correlation of $R^{2} = 0.9998$ (Fig~\ref{fig6}), which indicates that the expression profile of single cells produced by scSpark is reliable.
In addition, we also evaluated whether scSpark would have influence on downstream biological analysis.
Results show that the cell embeddings in tSNE map were highly consistent between scSpark and UMI-tools (Fig~\ref{fig7}).
This further demonstrates that the expression matrices generated by scSpark are accurate and would not bring interference to various types of downstream analysis such as clustering, dimension reduction and cell type identification.

\section{Conclusion}

In recent years, NGS methods proliferate the volume and value of scRNA-seq data, which requires more efficiency and scalability scRNASeq upstream pipelines.
Our scSpark is an efficiently, highly scalable and biological correct parallel compute scRNASeq upstream pipeline on the top of Spark.

In experiment, we believe our scSpark's strategy reduce the exection time with speed-up between 5 and 20 times in 64 CPU cores condition.
And scSpark can scale-out more than 256 CPU cores contrast with tradition pipelines' performance will converage when workstation scale-out 64 CPU cores.
Moreover, we found scSpark shows more scalability when FASTQ dataset volume increase which means scSpark have potential to processed increasing FASTQ dataset by making Spark cluster bigger.
Importantly, we also believe that scSpark result biological was confirmed.

\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
