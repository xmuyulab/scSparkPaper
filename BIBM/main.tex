\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\def\UrlBreaks{\do\A\do\B\do\C\do\D\do\E\do\F\do\G\do\H\do\I\do\J
\do\K\do\L\do\M\do\N\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V
\do\W\do\X\do\Y\do\Z\do\[\do\\\do\]\do\^\do\_\do\`\do\a\do\b
\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n
\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z
\do\.\do\@\do\\\do\/\do\!\do\_\do\|\do\;\do\>\do\]\do\)\do\,
\do\?\do\'\do+\do\=\do\#} 
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ScSpark: Low-Redundancy Disk Access and High-Performance Tool for the Single-Cell RNA Sequenceing Data Processing}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
  High-throughput single-cell RNA sequencing (scRNA-seq) data processing pipelines typically integrate multiple modules to transform raw scRNA-seq data to gene expression matrices, including barcode processing, sequence quality control, genome alignment and transcript counting. 
  With the drastic increase in scRNA-seq data size, file input/output (IO) has become a bottleneck of the processing speed of available tools. 
  In this study, we took advantage of Apache Spark's in-memory computing and inherent scalability to develop a new Java-based scRNA-seq data processing pipeline, named scSpark. 
  % We combined Spark and our proposed functionalities to implement sequence quality control and transcript counting. 
  To reduce unneccessary disk access while reading FASTQ files and writing SAM files, we used Java Native Interface (JNI) to deliver FASTQ Resilient Distributed Datasets (RDD), and then retrieved genome mapping results to SAM RDD. 
  By allocating scRNA-seq data and processing tasks to computer cluster, the scSpark toolkit can significantly reduce disk access for saving and loading temporary results. 
  To test the performance of scSpark, we built a spark cluster on Aliyun, and evaluated its computational performance and biological analysis robustness with several state-of-the-art data processing pipelines. The results indicate that scSpark is more efficient and more scalable than the other available tools. 
\end{abstract}

\begin{IEEEkeywords}
% Intermediate Data, Fast, Scalable
scRNA-seq data processing, Apache Spark, cloud computing
\end{IEEEkeywords}

\section{Introduction}
% Individual cells are the smallest unit of life, such as tissues and organs.
% According to the central dogma of molecular biology, one of the major factors that determines the function of every cell is its transcriptional program.
Single cell is the fundamental unit of a living organism.
In the era of precision medicine, exploring the gene expression at single cell level has become crucial.
In the past decade, RNA-seq has been widely used to study gene expression patterns in large-scale biological samples.
However, the resolution of bulk RNA-seq could only reach the average level of cell populations.
Single-cell RNA sequencing (scRNA-seq) essentially reveals the transcriptional status at single cell level, which provides the basis for subsequent bioinformatics analysis~\cite{Papalexi2018SinglecellRS}.

High-throughput (HT) scRNA-seq protocols, which enable transcript sequencing of thousands of cells simultaneously in a single experiment, have emerged as powerful tools to identify and characterize cell types in complex and heterogeneous tissues~\cite{Zhang2019ComparativeAO}.
% are the most popular strategies currently for scRNA-seq technology~\cite{Zhang2019ComparativeAO}. 
%Among them, Drop-seq~\cite{Macosko2015HighlyPG}, inDrop~\cite{Klein2015DropletBF} and 10X\cite{Zheng2017Massively} are the three most widely used protocols. 
To allow the reads to be demultiplexed after the cells being assembled for sequencing~\cite{Tian2018scPipe}, two important oligonucleotide barcodes, namely, cell barcode (CB) and unique molecular identifier (UMI) are introduced in scRNA-seq~\cite{Rosenberg2018SinglecellPO}~\cite{Cao2017ComprehensiveSC}. 
CB, which assigns different sequences to each cell for transcript source retrieval after sequencing, greatly improves the throughput and reduces the cost of scRNA-seq~\cite{Macosko2015HighlyPG}~\cite{Klein2015DropletBF}. 
In addition, UMIs are random oligonucleotide barcodes that are used in scRNA-seq experiments~\cite{Kivioja2012Counting}~\cite{Camara2017Methods} to distinguish redundant transcript generated from PCR amplification~\cite{Smith2017UMItools}.

In scRNA-seq experiment, it is the usage of these multi-level barcodes that presented additional challenges for data processing, which was quite different from traditional bulk RNA-seq and low-throughput scRNA-seq experiments. 
In recent years, researchers have developed multiple scRNA-seq data processing tools, typically implementing steps including barcode processing, sequence quality control, genome alignment and transcript counting to convert raw scRNA-seq data into a gene expression matrix for further downstream analysis. 
The first step is to extract barcodes from a pre-designed nucleotide site for different scRNA-seq protocols. Then, FASTQ reads with low quality nucleotides are filtered based on user-defined thresholds. 
%This step eliminates the errors in CBs, and greatly improves the accuracy of the CBs that need to be considered for transcript counting. 
Subsequently, the remaining FASTQ reads are mapped to the reference genome using the splice-aware aligner, such as STAR~\cite{Dobin2013STAR} or HISAT2~\cite{Kim2015HISAT}. 
%Among them, STAR is the most widely used. A number of popular scRNA-seq data processing tools use STAR for mapping, such as UMI-Tools, CellRanger, zUMIs, scPipe, and STARsolo. 
%Previous baseline studies have shown that STAR is one of the most reliable reference genomic aligner for RNA-Seq analysis~\cite{Baruzzo2017SimulationbasedCB}. 
Finally, the reads are assigned to genes and count matrices for UMIs are generated.~\cite{Parekh2018zUMIs}. 

Nowadays, there are many tools to preprocess scRNA-seq data, among which the most popular ones inlude CellRanger~\cite{Zheng2017Massively}, UMI-tools~\cite{Smith2017UMItools},  STARsolo~\cite{Blibaum2019STARsolo}, etc. 
CellRanger is a highly integrated data processing software tool tailored by 10X Genomics for scRNA-seq data analysis, and it presented best performance on the machine with enough CPU and memory resources for big datasets, while performed slightly poor for small datasets~\cite{Gao2020Comparison}. 
%CellRanger performs best under large data sets among existing tools~\cite{Gao2020Comparison}.
In addition, UMI-tools~(\url{https://github.com/CGATOxford/UMI-tools}) is an open source tool with an impressively clear process. And it had relatively higher trascript quantification accuracy compared with other tools in a previous evalution~\cite{Gao2020Comparison}. 
Furthermore, STARsolo~\cite{Blibaum2019STARsolo} has improved the parallelism of sequence mapping and counting procedures, and achieved good performance on a single machine.
Great improvement has been achieved recently for the development of scRNA-seq data processing tools. However, all the avaliable scRNA-seq data processing tools can only run on a single machine with limited running speed, and cannot be extended to multiple nodes.

In recent years, there have been many studies focusing on the optimization of next generation sequencing (NGS) data processing using big data framework like Hadoop~(\url{https://hadoop.apache.org}) and Spark~(\url{https://spark.apache.org}). 
Due to higher efficiency in utilizing memory computing, Spark is a better big data computing framework than Hadoop's MapReduce~\cite{Dean2008MapReduce, Zaharia2012Resilient}. 
SparkBWA~\cite{Abun2016SparkBWA} and GPF~\cite{Li2018Highperformance} are good frameworks with Spart for improving the efficiency of NGS data processing. 
Falco~\cite{Yang2017Falco} has tried to use Spark in scRNA-seq data preprocessing, but it has two limitations. 

Firstly, Falco can't deal with CBs and UMIs, so it is incompatible with any HT scRNA-seq protocol.
Secondly, Falco only uses Spark to simply concatenate the aligner and the feature quantitative softwares, which does not reduce the amount of disk reads and writes during alignment and transcript counting.
Falco’s operations have lots of redundancy disk access which causes its insufficient utilization of Spark’s in-memory computation well. 

The increase of scRNA-seq datasets require more efficient and faster data processing tools. However, the current scRNA-seq data preprocessing tools were designed without considering of the scalability, which could only run on a single computer and could not be extended to a cluster. From UMI-Tools and CellRanger to STARsolo, parallelism and performance has increased; However, due to the limit of a single machine, all of them have no scalability . 
The traditional single machine serial processing mode has been unable to meet the demand of computing and storage resources for super-large scale scRNA-seq data.
Different with Falco, we enhanced STAR's file I/O interface to reduce redundant disk access.
In the end, to achieve parallel and in-memory computing, scSpark implemented transcript counting by combining Spark function and our method that can distributed cumpute the result and generated by a graph algorithm.
Our work utilized Spark system architecture to develop a quick and scalable scRNA-seq data preprocessing tool. 

\section{Method}
\subsection{Overview}
ScSpark implemented cell barcode and UMI correct, sequence quality control, genome alignment, and transcript counting based on Spark framework and UMI-tools.
ScSpark run program across Spark's cluster and default cache data in memory. 
We implemented cell barcode and UMI correct when Spark's cluster load data and abstract them to RDD.
And we developed a high concurrency Spark version sequence quality control. 
After that, scSpark processed STAR's program on each node in the cluster.
In the end, scSpark grouped each cell's record and distributed conut the result.

\subsection{Simple barcode and UMI correct}
We override FASTQ hadoop loading api~(\url{https://github.com/HadoopGenomics}).
Large FASTQ files can be loaded in each node's memory parallel.
So scSpark's loading speed doesn't limit by single node disk access speed.
In this step, scSpark splits barcode and UMI when scSpark load FASTQ file to FASTQ RDD.
ScSpark splits FASTQ R1's sequence to two part, uses barcode as RDD's key and uses others information as RDD's value.

\subsection{Spark version sequence quality control}
\begin{figure}
	\includegraphics[width=0.45\textwidth]{Fig1.pdf}
	\caption{An overview of sequence quality control module based on Spark.} \label{fig1}
\end{figure}
As shown in Fig~\ref{fig1}, the sequence quality control step typically consists of two main components.
First, generating whitelist RDD including identities of the true cell barcodes is unknown.
After FASTQ R1 RDD generated, we calculated the number of occurences for each CB and selected the most frequent CBs, named whitelist RDD, as intermediate results. 
By using reduceByKey function, each cell barcode could be counted parallelly in each split of FASTQ R1 RDD and we could merge each split's result in the end. 
After that, we can use the result to generate whitelist RDD by Spark's sort and take function, both of which can parallelly compute in the cluster.
Then scSpark can easily uses whitelist RDD to filter cached FASTQ R1 RDD by using Spark's join function to generate extracted FASTQ R1 RDD.
In the time to filter FASTQ R2 RDD, scSpark uses same strategies again. 
It uses extracted FASTQ R1 RDD to filter FASTQ R2 RDD when they have same line index.
The last step in sequence quality control was using join function again to combine filtered FASTQ R1 RDD and FASTQ R2 RDD with the same index. 

\subsection{Eleminate redundancy disk access in aligner step}
\begin{figure}
	\includegraphics[width=0.45\textwidth]{Fig2.pdf}
	\caption{New way to implement STAR's interface.} \label{fig2}
\end{figure}
% Apart from Cellranger we can't know how it implements because it isn't an open source software. 
In previous pipeline, STAR needs to load extracted FASTQ R2 file and FASTQ R1 file.
This way will generate unneccessary disk access. 
We chose STAR as our aligner and modified STAR's loading FASTQ way to avoid loading extracted FASTQ R2 and FASTQ R1 twice. 
As shown in Fig~\ref{fig2}, we utilized Java Native Interface to transfer extracted FASTQ R2 RDD's data to STAR program, and then run the STAR process.
The STAR aligner was encapsulated as a shared object and invoked the shared object as our aligner. 
To overcome imbalance problem of each node's data volume, we repartitioned the extracted FASTQ R2 RDD before transfer FASTQ RDD to STAR.
Each node could run STAR parallelly with counterpoise data volume. 
We also found if node's in memory was enough, we could start up more than one STAR's in one node to achieve better parallel computing.
After align, STAR's output is SAM file or BAM file that will generate more disk access waste than STAR's input.
Here, We used Java Native Interface again to transfer STAR results and directly abstract them as SAM RDD. 

\subsection{Count with multi-node}
\begin{figure}
	\includegraphics[width=0.45\textwidth]{Fig3.pdf}
	\caption{An overview of Spark version count.} \label{fig3}
\end{figure}
As shown in Fig~\ref{fig3}, to take advantage of multi-node's compute capability, we used a new way to implement the count step. 
Except for SAM RDD that were generated in the previous step, we loaded GTF file to memory and abstract them as GTF RDD. 
After that scSpark grouped SAM RDD and GTF RDD according to their cell name. 
Then we used flatMap function to parallelly compute each cell's read count. 
In the end, we collected results in all nodes, and the result files would produce the only output disk access in the whole program. 
ScSpark breaks the limitation of one node's computing and achieved large scalability. 

\subsection{Experiment design}
We used Apache Spark version 2.1.0 as scSpark's in memory computing environment.
And three example scRNA-seq datasets generated by 10X Genomics on their platform was used in our experiments.
Three datasets all come from 10xgenomics datasets~(\url{support.10xgenomics.com}).
Dataset1, 10X Genomics v3 10k peripheral blood mononuclear cell (PBMC) from a healthy donor, which contain 640 millions records.
Dataset2, 10X Genomics 10k Rat PBMCs Multiplexed from a  Wistar rat strain~(\url{www.10xgenomics.com/resources/datasets/10-k-rat-pbm-cs-multiplexed-2-cm-os-3-1-standard-6-0-0}), which contain 289.3 millions records.
Dataset3, 10X Genomics 10k Monkey PBMCs Multiplexed from a Rhesus monkey strain~(\url{www.10xgenomics.com/resources/datasets/10-k-monkey-pbm-cs-multiplexed-2-cm-os-3-1-standard-6-0-0}), which contain 261.5 millions records.
To compare with tradition pipelines, we tested three datasets in UMI-tools, CellRanger, STARsolo and scSpark's performance.
Limited by single machine's CPU cores, we only take 64 CPU cores in traditional pipeline test as the traditional pipeline's performance.
And then, we tested each step's process spend time, due to CellRanger isn't an open source software.
And STARsolo implemented pipeline in a different way, we choose UMI-tools as scSpark each step's performance baseline.
After that, in align step, four pipelines' align step all based on STAR's program.
So we tested STAR and scSpark's mapping speed to prove scSpark not only get greatly improve in same CPU cores compare with STAR, but also can get near linear improve when CPU cores increase.
Except performance evaluate, we use 16 CPU cores as tradition pipeline's scalability baseline and use 64 CPU cores as scSpark's scalability baseline to prove scSpark's scalability better than tradition pipelines.
ScSpark is developed based on UMI-tools. 
And UMI-tools accuracy was fully verified. 
This section we used the gene expression matrix that was obtained by scSpark and UMI-tools under the same dataset to perform downstrema analysis of scRNA-seq data. 
And then we compared two tools transcript analysis's result and cell cluster analysis's result to verify the correlation between scSpark and UMI-tools. 
Under hgmm-1k-v3 dataset, we used scSpark and UMI-tools to get gene express matrix, compared two tools' result, and computed their correlation.

\section{Results}
First, we compared each pipeline's speed.
And then we compared each step's speed of scSpark with UMI\_tools.
After that, we contrasted STAR's mapping speed in different CPU cores.
We also discussed each pipeline's scalability in this section.
\subsection{Efficiency evaluation}
\begin{table}
	\centering
	\caption{previous pipeline and scSpark's performance comparisione}\label{tab1}
	\resizebox{0.45\textwidth}{!} {
	\begin{tabular}{l | l | l | l  l}
		\hline
		System & Dataset & Cores & Spend time(s) \\
		%\hline
		& & & 100M reads & 640M reads  \\
		\hline
		UMI-tools & 1 & 64 cores & 7254 & 44160 \\
		CellRanger & 1 & 64 cores & 6000 & 11700 \\
		STAR-solo & 1 & 64 cores &  5820 & 8100 \\
		scSpark & 1 & 4*16 cores & 355 & - \\
		& 1 & 16*16 cores & - & 841 \\
		UMI-tools & 2 & 64 cores & 7254 & 44160 \\
		CellRanger & 2 & 64 cores & 6000 & 11700 \\
		STAR-solo & 2 & 64 cores &  5820 & 8100 \\
		scSpark & 2 & 4*16 cores & 326 & - \\
		& 2 & 16*16 cores & - & 841 \\
		UMI-tools & 3 & 64 cores & 7254 & 44160 \\
		CellRanger & 3 & 64 cores & 6000 & 11700 \\
		STAR-solo & 3 & 64 cores &  5820 & 8100 \\
		scSpark & 3 & 4*16 cores & 391 & - \\
		& 3 & 16*16 cores & - & 841 \\
		\hline
	\end{tabular}
	}
\end{table}
Table~\ref{tab1} gives a summary of scSpark's performance and compare with tradition pipeline's performance.
We can see scSpark's speed is much quicker than any tradition pipelines in same CPU cores environment.
And scSpark can get improve when the cluster's CPU cores number increase.
\begin{table}
	\centering
	\caption{Performance comparision for each step}\label{tab2}
	\resizebox{0.45\textwidth}{!} {
	\begin{tabular}{l | l | l | l | l | l}
		\hline
		System & Dataset & Cores & Barcode and UMI correct, quality control & align & count \\
		\hline
		UMI-tools(100 million reads) & 1 & 64 cores & 9720 & 600 & 1740 \\
		UMI-tools(640 million reads) & 1 & 64 cores & 33600 & 2160 & 8400 \\
		scSpark(100 million reads) & 1 & 4*16 cores & 130 & 132 & 93 \\
		scSpark(640 million reads) & 1 & 16*16 cores & 81 & 447 & 313 \\
		UMI-tools(100 million reads) & 2 & 64 cores & 9720 & 600 & 1740 \\
		UMI-tools(289.3 million reads) & 2 & 64 cores & 33600 & 2160 & 8400 \\
		scSpark(100 million reads) & 2 & 4*16 cores & 137 & 117 & 72 \\
		scSpark(289.3 million reads) & 2 & 16*16 cores & 81 & 447 & 313 \\
		UMI-tools(100 million reads) & 3 & 64 cores & 9720 & 600 & 1740 \\
		UMI-tools(261.5 million reads) & 3 & 64 cores & 33600 & 2160 & 8400 \\
		scSpark(100 million reads) & 3 & 4*16 cores & 129 & 227 & 35 \\
		scSpark(261.5 million reads) & 3 & 16*16 cores & 81 & 447 & 313 \\
		\hline
	\end{tabular} }
\end{table}

We recorded each step's process time to find the reason of scSpark's speed improvement.
As table~\ref{tab2} shown, scSpark is much faster than the UMI-tools in any single step.

Contrast with UMI-tools, scSpark gets great improve in the first two steps, barcode correct and quality control.
This step's improve comes from using in memory trait to reduce disk access times and take advantage of Spark's high concurrency feature.
% scSpark's mapping speed increase with CPU cores number picture(need add in 2021.9.5)

After that, due to using in memory FASTQ RDD and SAM RDD to replace tradition pipeline's disk access, scSpark's align step can get great improve in same CPU cores condition.
% STAR's mapping speed picture
\begin{table}
	\centering
	\caption{STAR's mapping speed}\label{tab3}
	\resizebox{0.45\textwidth}{!} {
	\begin{tabular}{l | l | l | l | l | l | l}
		\hline
		 & 2 cores & 4 cores & 8 cores & 16 cores & 32 cores & 64 cores \\
		\hline
		Dataset1 & 120.97 & 242.08 & 449.09 & 772.13 & 1389.32 & 961.59 \\
		Dataset2 &  & 261.23 & 530.57 & 1089 & 1391.24 & 1347.35 \\
		Dataset3 & 38.7 & 76.38 & 170.01 & 325.85 & 512.59 & 828.99 \\
		\hline
	\end{tabular} }
\end{table}
Furthermore, we also found STAR's mapping speed can get improve along with CPU's cores increase, but will converge after touching the threshold.
Although scSpark's align step based on STAR's program, scSpark's mapping speed not only have greater performance in same CPU cores but also can get nearly linear imporve when cluster's CPU's cores number improve.
But scSpark takes advantage of Spark's distributed trait, processes multi STAR's program in same time, and each STAR's can fully utilize multi thread ability.

After align step, we grouped records by cell and concurrency processed each group in each CPU cores.
This way also gets great improve compare with tradition pipeline.

\subsection{scalability evaluation}
Contrast with tradition pipelines, scSpark not only can perform well in same CPU cores but also can get improve when CPU cores number increase.
% CPU数增加对传统流程性能影响小
As shown in Fig 6, we found tradition pipelines can get improve when CPU cores number increase but will converge quickly.
And we found scSpark's scalability is much better than tradition pipeline.
Moreover we found scSpark's scalability will increase with data volume.
% scSpark不同核心数不同步骤的表现
As shown in Fig 7, scSpark's scalability major came from align step and align step consumes major time of scSpark.
To find the reason why the align step's scalability influenced by data volume, we tested STAR's mapping speed in different data volume.
\begin{table}
  \centering
  \caption{Processing speed w.r.t. data volume}\label{tab3}
	\begin{tabular}{|l | l | l | l | l|}
	\hline
	Volume (million reads) & 10 & 50 & 100 & 640 \\
	\hline
	Speed (million reads per hour) & 250.28 & 503.83 & 503.02 & 950 \\
	\hline
  \end{tabular}
\end{table}
As Table~\ref{tab3} shown, STAR mapping speed will increase when the data volume increase. 
ScSpark align step is based on STAR, when cluster's CPU cores increase, each node's STAR program's data volume decrease.
So in align step, scSpark's scalability decreases when each node's data volume less than a threshold.

\subsection{Performance Analysis}
We tested performance to find bottleneck of our scSpark.
We used 640 millions records of FASTQ data to evaluate two aspect of our scSpark.
First we compared scSpark spend in network shuffle, disk access and compute time proportion to find whether network shuffle or disk access occupies too much time.
Second we tested CPU and memory usage, to ensure which resources will be scSpark's bottleneck.
  
\subsubsection{Network and disk behavior}
\begin{figure}
  \includegraphics[width=0.5\textwidth]{fig7.pdf}
  \caption{Each step consumes time.} \label{fig7}
\end{figure}
We computed network time by summing up the time that our scSpark shuffle data in multi machine. 
Disk access comes from loading FASTQ, STAR's index and GTF files. 
The ideal situation is tasks did not waste any time in disk access and network shuffle. 
We found that scSpark's computing time occupies most execute time. 
Except STAR's index file, all file's disk access distribute to each node that improve whole system's loading speed. 
And we found the time that waste in shuffling doesn't occypy too much time. 
  
\subsubsection{CPU and memory usage}
\begin{figure}
  \includegraphics[width=0.5\textwidth]{fig8.pdf}
  \caption{CPU and Memory usage of scSpark.} \label{fig8}
\end{figure}
We monitored scSpark's CPU and memory usage during processing. 
As Fig~\ref{fig8} shown, in sequence quality control step, scSpark highly exploited each node's multi cores CPU to achieve speedup. 
Convenition pipelines single thread solution's CPU usage is much lower than scSpark. 
And we found that scSpark's boundary much comes from genome alignment.
Because we invoked STAR as our alignment tool, and STAR's program naturally occupy most proportion of memory in this step.
  
\subsubsection{Biological verification} 
As Fig~\ref{fig9} shown, for each after processing cell barcode, their gene expression matrix approximate fit $y=x$, $R^{2}$ closed to 0.9998. 
\begin{figure}
  \includegraphics[width=0.5\textwidth]{fig9.pdf}
  \caption{correlation of scSpark and UMI-tools.} \label{fig9}
\end{figure}
Furthermore, we used Seurat to print tSNE picture which based on gene expression matrix generated by ScSpark and UMI-tools. 
And as Fig~\ref{fig10} shown, tSNE cell clustering result showed high corrleation between ScSpark and UMI-tools. 
\begin{figure*}
  \includegraphics[width=\textwidth]{fig10.pdf}
  \caption{tSNE picture based on scSpark and UMI-tools' gene expression matrix.} \label{fig10}
\end{figure*}

\section{Conclusion}
In this paper, we proposed a way which utilize Apache Spark's in memory compute and distributed compute trait to strengthen upstream scRNA-seq pipeline.
We developed scSpark, which can get considerable speedup and scalability compared with tradition pipelines.

Refer to UMI-tools, scSpark divided the upstream process into four steps, UMI and CB correct, sequence quality control, genome alignment and transcript counting.
And we found scSpark's each steps can get higher performance than UMI-tools.

\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
