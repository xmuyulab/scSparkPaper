\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\def\UrlBreaks{\do\A\do\B\do\C\do\D\do\E\do\F\do\G\do\H\do\I\do\J
\do\K\do\L\do\M\do\N\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V
\do\W\do\X\do\Y\do\Z\do\[\do\\\do\]\do\^\do\_\do\`\do\a\do\b
\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n
\do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z
\do\.\do\@\do\\\do\/\do\!\do\_\do\|\do\;\do\>\do\]\do\)\do\,
\do\?\do\'\do+\do\=\do\#} 
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ScSpark: High-Performance scRNA-seq Data Processing with Low-Redundancy Disk Access}

\author{\IEEEauthorblockN{Yu Liu}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
liuyu123@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Mingxuan Gao}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
mingxuangao@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Lixuan Tan}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
tanlix@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Hongjin Liu}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
liuhongjin@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Yating Lin}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
linyating@stu.xmu.edu.cn}
\and
\IEEEauthorblockN{Wenxian Yang}
\IEEEauthorblockA{\textit{Aginome Scientific} \\
Xiamen, China \\
wx@aginome.com}
\and
\IEEEauthorblockN{Rongshan Yu*}
\IEEEauthorblockA{\textit{School of Informatics} \\
\textit{Xiamen University}\\
Xiamen, China \\
rsyu@xmu.edu.cn}
}

\maketitle

\begin{abstract}
%Single-cell RNA-seq (scRNA-seq) is increasingly used in a range of biomedical studies. Nonetheless, current RNA-seq analysis tools are not specifically designed to efficiently process scRNAseq data due to their limited scalability
High-throughput single-cell RNA sequencing (scRNA-seq) data processing pipelines integrate multiple modules to transform raw scRNA-seq data to gene expression matrices, including barcode processing, sequence quality control, genome alignment and transcript counting.
Due to increasingly data volume of a scRNA-seq study, the speed of scRNA-seq data processing pipeline has become a major bottleneck to scRNA-seq study. 
%  In this study, we took advantage of Apache Spark's in-memory computing and inherent scalability to develop a new Java-based scRNA-seq data processing pipeline, named scSpark. 
%Here we introduce Falco, a cloud-based framework to enable paralellisation of existing RNA-seq processing pipelines using big data technologies of Apache Hadoop and Apache Spark for performing massively parallel analysis of large scale transcriptomic data. 
In this study, we developed scSpark, a cloud-computing based scRNA-seq processing pipeline. 
% We combined Spark and our proposed functionalities to implement sequence quality control and transcript counting. 
%To reduce unnecessary disk access while reading FASTQ files and writing SAM files, we used the Java Native Interface (JNI) to deliver FASTQ Resilient Distributed Datasets (RDD), and retrieved genome mapping resul ts to SAM RDD. 
%Using two public scRNA-seq data sets and two popular RNA-seq alignment/feature quantification pipelines, we show that the same processing pipeline runs 2.6 – 145.4 times faster using Falco than running on a highly optimised single node analysis.
By leveraging Apache Spark's in-memory computing capability, scSpark significantly increases the processing speed of scRNA-seq data, and achieves 5\-20 times faster than state-of-the-art processing pipelines under the same CPU core consumption.
In addition, due to Spark's inherent scalability in a cloud-computing environment, scSpark can further decrease the processing time for a typical scRNAseq dataset (e.g., 640 million reads) from hours to minutes when multiple computer nodes (e.g., 16) are used.  
Biological evaluation also confirmed that the results produced by scSpark is highly consistent with existing scRNAseq pipelines.
\end{abstract}

\begin{IEEEkeywords}
% Intermediate Data, Fast, Scalable
scRNA-seq data processing, Apache Spark, cloud computing
\end{IEEEkeywords}

\section{Introduction}
Single cell is the fundamental unit of a living organism.
Historically, RNA-seq has been widely used to study gene expression patterns in biological samples.
However, the resolution of bulk RNA-seq could only reach the average level of cell populations. 
With the development of single-cell sequencing technologies, scRNA-seq now allows transcript sequencing of thousands of cells simultaneously in a single experiment, and has emerged as powerful tools to identify and characterize cell types in complex and heterogeneous tissues~\cite{Zhang2019ComparativeAO}

%In order to demultiplex the reads for inidividual cells after sequencing~\cite{Tian2018scPipe}, two oligonucleotide barcodes, the cell barcode (CB) and the unique molecular identifier (UMI), have been introduced in single cell sequencing~\cite{Rosenberg2018SinglecellPO,Cao2017ComprehensiveSC}.  When using the CB, different barcode sequences are assigned to each cell for transcript source retrieval after sequencing, thus greatly improving the throughput and reducing the cost of scRNA-seq~\cite{Macosko2015HighlyPG,Klein2015DropletBF}. UMIs are random oligonucleotide barcodes used in single cell sequencing to distinguish redundant transcripts generated from PCR amplification~\cite{Kivioja2012Counting,Camara2017Methods,Smith2017UMItools}. 

To enable efficient bioinformatics processing of sequencing data produced from scRNA-seq studies, various scRNA-seq data processing pipelines have been developed.
A fully-functioned scRNA-seq data processing pipeline typically implements multiple modules including unique molecular identifier (UMI) barcode~\cite{Smith2017UMItools} processing, sequence quality control (QC)~\cite{schmieder2011quality}, genome alignment~\cite{Dobin2013STAR,Kim2015HISAT} and transcript quantification~\cite{Parekh2018zUMIs} to convert raw scRNA-seq data into a gene expression matrix for further downstream analysis. 
%First, the barcodes are extracted from a pre-designed nucleotide site for different scRNA-seq protocols. In QC, the FASTQ reads with low quality nucleotides are filtered based on user-defined thresholds.  Subsequently, the remaining FASTQ reads are mapped to the reference genome using a splice-aware aligner, such as STAR~\cite{Dobin2013STAR} and HISAT2~\cite{Kim2015HISAT}. Finally, the reads are assigned to genes and the count matrices for UMIs are generated~\cite{Parekh2018zUMIs}. 
Among all the scRNA-seq data processing pipelines, the most influential studies probably include CellRanger~\cite{Zheng2017Massively}, UMI-tools~\cite{Smith2017UMItools}, and STARsolo~\cite{Blibaum2019STARsolo}, etc. 
CellRanger is a highly integrated data processing software tool tailored by 10X Genomics for scRNA-seq data analysis. It is most suitable for processing large datasets on high performance workstations~\cite{Gao2020Comparison}. 
UMI-tools is a comprehensive scRNA-seq data processing suite with the directional barcode collapse algorithm integrated that cosiderably promotes the transcript quantification accuracy.
STARsolo is a recently developed extended pipeline based on the original genome aligner STAR to adapt the single-cell applications. 
However, these scRNA-seq data processing pipelines lack the scalability to meet the computational demand from the large volume of scRNA-seq data from increasing amount of single-cell studies. 

%super-large scale scRNA-seq data

Recently, big data frameworks such as Apache Hadoop (\url{https://hadoop.apache.org}) and Apache Spark (\url{https://spark.apache.org}) have been used to speed up the data processing for the next generation sequencing (NGS) data. 
SparkBWA~\cite{Abun2016SparkBWA} exploits the capabilities of Spark to boost the performance of one of the most widely adopted aligner for NGS data, the Burrows-Wheeler Aligner (BWA). 
GPF~\cite{Li2018Highperformance} is a fast in-memory computing framework implemented based on Spark for implementing NGS data processing pipelines in a cloud-computing environment. 
Following the success of cloud-based implementation of NGS data processing pipelines, Falco~\cite{Yang2017Falco} concatenates the read aligner and transcript quantification software tools using Spark for scRNA-seq data preprocessing.
Although Falco improves the performance of scRNA-seq data processing when a distributed computing environment is available, it doesn't support unique molecular identifier (UMI) barcode processing. Hence, it is incompatible with the high-throughput scRNA-seq protocols such as 10X Genomics, Drop-seq and Microwell-seq. In addition, it doesn't leverage the in-memory computing capability of Spark to reduce the disk read/write operations of intermediate software processing steps for further performance speedup.  

Herein, we present scSpark, a scRNA-seq data processing pipeline using the Spark framework for high performance data processing in cloud computing environment. 
To meet the computational demand from processing large volume of scRNA-seq data, 
we developed scSpark, a high-performance, scalable, in-memory scRNA-seq data processing pipeline to support large-scale scRNA-seq data processing. More specifically, 
by implementing and integrating multiple bioinformatic processing steps of a standard scRNA-seq processing pipeline with the in-memory computing framework of Spark, scSpark eliminates the need for laborious disk read/write operations associated with traditional bioinformatics tools.
As a result, scSpark is able to improve the scRNAseq data processing speed by more than ten-folds compared to other state-of-the-art software tools under the same CPU core consumption. In addition, by leveraging the parallel computing capability of Spark engine, scSpark further enables users to distribute their scRNA-seq data processing workloads to multiple computational nodes, thus dramatically increases their processing throughput of scRNA-seq data for large-scale studies. 

ScSpark is freely obtainable: https://github.com/xmuyulab/
spark-scRNASeq-Analysis.git.
After Introduction, we provides method to implement scSpark.
Finally we evaluates scSpark on a 256 (16*16) cores compute cluster and analyzes scSpark's performance and scalability before concluding in the end.


\iffalse
% Individual cells are the smallest unit of life, such as tissues and organs.
% According to the central dogma of molecular biology, one of the major factors that determines the function of every cell is its transcriptional program.

% are the most popular strategies currently for scRNA-seq technology~\cite{Zhang2019ComparativeAO}. 
%Among them, Drop-seq~\cite{Macosko2015HighlyPG}, inDrop~\cite{Klein2015DropletBF} and 10X\cite{Zheng2017Massively} are the three most widely used protocols. 
In scRNA-seq experiment, it is the usage of these multi-level barcodes that presented additional challenges for data processing, which was quite different from traditional bulk RNA-seq and low-throughput scRNA-seq experiments. 

The first step is to extract barcodes from a pre-designed nucleotide site for different scRNA-seq protocols. Then, FASTQ reads with low quality nucleotides are filtered based on user-defined thresholds. 
%This step eliminates the errors in CBs, and greatly improves the accuracy of the CBs that need to be considered for transcript counting. 
Subsequently, the remaining FASTQ reads are mapped to the reference genome using the splice-aware aligner, such as STAR~\cite{Dobin2013STAR} or HISAT2~\cite{Kim2015HISAT}. 
%Among them, STAR is the most widely used. A number of popular scRNA-seq data processing tools use STAR for mapping, such as UMI-Tools, CellRanger, zUMIs, scPipe, and STARsolo. 
%Previous baseline studies have shown that STAR is one of the most reliable reference genomic aligner for RNA-Seq analysis~\cite{Baruzzo2017SimulationbasedCB}. 
Finally, the reads are assigned to genes and count matrices for UMIs are generated.~\cite{Parekh2018zUMIs}. 

%This step eliminates the errors in CBs, and greatly improves the accuracy of the CBs that need to be considered for transcript counting. 
%Among them, STAR is the most widely used. A number of popular scRNA-seq data processing tools use STAR for mapping, such as UMI-Tools, CellRanger, zUMIs, scPipe, and STARsolo. 
%Previous baseline studies have shown that STAR is one of the most reliable reference genomic aligner for RNA-Seq analysis~\cite{Baruzzo2017SimulationbasedCB}. 

CellRanger performs best under large data sets among existing tools~\cite{Gao2020Comparison}.
In addition, UMI-tools (\url{https://github.com/CGATOxford/UMI-tools}) is an open source tool with an impressively clear process. And it had relatively higher trascript quantification accuracy compared with other tools in a previous evalution~\cite{Gao2020Comparison}. 
Furthermore, STARsolo~\cite{Blibaum2019STARsolo} has improved the parallelism of sequence mapping and counting procedures, and achieved good performance on a single machine.
Great improvement has been achieved recently for the development of scRNA-seq data processing tools. However, all the avaliable scRNA-seq data processing tools can only run on a single machine with limited running speed, and cannot be extended to multiple nodes.

In recent years, there have been many studies focusing on the optimization of next generation sequencing (NGS) data processing using big data framework like Hadoop (\url{https://hadoop.apache.org}) and Spark (\url{https://spark.apache.org}). 
Due to higher efficiency in utilizing memory computing, Spark is a better big data computing framework than Hadoop's MapReduce~\cite{Dean2008MapReduce, zaharia2010spark, Zaharia2012Resilient}. 
SparkBWA~\cite{Abun2016SparkBWA} and GPF~\cite{Li2018Highperformance} are good frameworks with Spart for improving the efficiency of NGS data processing. 
Falco~\cite{Yang2017Falco} has tried to use Spark in scRNA-seq data preprocessing, but it has two limitations. 

Firstly, Falco can't deal with CBs and UMIs, so it is incompatible with any high-throughput scRNA-seq protocol.
Secondly, Falco only uses Spark to simply concatenate the aligner and the feature quantitative softwares, which does not reduce the amount of disk reads and writes during alignment and transcript counting.
Falco’s operations have lots of redundancy disk access which causes its insufficient utilization of Spark’s in-memory computation well. 

The increase of scRNA-seq datasets require more efficient and faster data processing tools.
However, the current scRNA-seq data preprocessing tools were designed without considering of the scalability, which could only run on a single computer and could not be extended to a cluster. From UMI-Tools and CellRanger to STARsolo, parallelism and performance has increased; However, due to the limit of a single machine, all of them have no scalability . 
The traditional single machine serial processing mode has been unable to meet the demand of computing and storage resources for super-large scale scRNA-seq data.
Different with Falco, we enhanced STAR's file I/O interface to reduce redundant disk access.
In the end, to achieve parallel and in-memory computing, scSpark implemented transcript counting by combining Spark function and our method that can distributed cumpute the result and generated by a graph algorithm.
Our work utilized Spark system architecture to develop a quick and scalable scRNA-seq data preprocessing tool. 

\fi


\section{Method}
\subsection{Overview}

ScSpark is designed on the top of the Spark~\cite{zaharia2010spark} framework.
% implemented refered to UMI\-tools, including barcode extraction, sequence quality control, genome alignment, and transcript counting, and is built on the top of Spark framework. 
% ref：We design our engine on top of the Spark framework, which by default keeps persistent RDDs in memory. 
Therefore scSpark inherently take advantage of Spark's trait including distribute compute and default keeps persistent Resilient Distributed Datasets (RDDs)~\cite{Zaharia2012Resilient} in memory.
% ref: This paper is focused on sharing our experiences designing a Spark-based pipeline for GATK.
% ref: There are three major steps in this reads-to-variant pipeline shown in Figure 1: 
% ref: (i) align reads to a reference or genome (Aligner),
% ref: (ii) manipulating the read alignments by sorting/indexing and eliminating dupli- cates (Cleaner) and 
% ref: (iii) variant discovery and genotyping (Caller).
% ref: Here we explain the major conceptual and general applications for each category.
ScSpark's algorithm refers to UMI\-tools.
% including barcode extraction, sequence quality control, genome alignment and transcipt counting.
And we split these steps into three major parts: (i) load FASTQ files, extract barcode and sequnence quality control, (ii) align extracted FASTQ reads to a reference or genome, (iii) quantify transcript.
% The data loading will automatically distribute the reads to the nodes in the cluster. %, and the reads that belong to the same cell will be sent to the same node, thanks to the cell barcodes (I don't think this is correct). 
% A high concurrency sequence quality control method designed for the Spark framework is then applied to filter the reads. 
% After that, STAR is applied on each node in the cluster for sequence alignment. 
% Finally, the alignment results from all nodes are aggregated and the cell count matrix is generated. 
Next we explain scSpark's way to implement each part and the way to evaluate our scSpark.

\iffalse
ScSpark implemented barcode extraction, sequence quality control, genome alignment, and transcript counting based on Spark framework and UMI-tools.
ScSpark run program across Spark's cluster and default cache data in in-memory. 
We implemented cell barcode and UMI correct when Spark's cluster load data and abstract them to RDD.
And we developed a high concurrency Spark version sequence quality control. 
After that, scSpark processed STAR's program on each node in the cluster.
In the end, scSpark grouped each cell's record and distributed conut the result.
\fi

\subsection{FASTQ data loader, barcode extraction and Sequence quality control using Spark}
\begin{figure}
	\includegraphics[width=0.45\textwidth]{figure1.png}
	\caption{An overview of sequence quality control module in scSpark.} \label{fig1}
\end{figure}
As Fig.~\ref{fig1} shown, to overcome inefficiency of tradition pipeline's intensity I/O, scSpark used Spark as FASTQ file data loader.
ScSpark's FASTQ data loader based on Hadoop-BAM\cite{hadoopBAM}, can easily transfer FASTQ reads to FASTQ RDDs. 
During loading data, scSpark extracts cell barcodes from reads, and transfer these reads to FASTQ RDD in key-value structure, all in a distributed manner. 
As multiple node computers read FASTQ files in parallel, the data loading speed is not limited by the access speed of a harddisk. 
% RDD可以是(key, value)（像数据结构中的map）这样的格式，为了下一步的处理，将FASTQ R1 RDD处理为(R1 cell barcode, R1 read name + R1 UMI + R1第四行)这样的格式
% Note that for paired-end reads, (ScSpark splits FASTQ R1's sequence to two part, uses barcode as RDD's key and uses others information as RDD's value.???)

\iffalse
We override FASTQ hadoop loading api~(\url{https://github.com/HadoopGenomics}).
Large FASTQ files can be loaded in each node's memory parallel.
So scSpark's loading speed doesn't limit by single node disk access speed.
In this step, scSpark splits barcode and UMI when scSpark load FASTQ file to FASTQ RDD.
ScSpark splits FASTQ R1's sequence to two part, uses barcode as RDD's key and uses others information as RDD's value.
\fi

% After that, from the FASTQ R1 RDD, we calculated the number of occurrences for each barcode and consider the most frequent barcodes as high confidence cell barcodes.
% This whitelist of barcodes is saved as an RDD object. 
After that, we achieved whitelist RDD from FASTQ R1 RDD by using topN algorithm in FASTQ R1's cell barcode~\cite{guo2018bioinformatics} and user can easily set the threshold.
% By using the reduceByKey function of Spark, each cell barcode can be counted in parallel in each split of FASTQ R1 RDD. The counts of the cell barcodes from different nodes are merged. 
% After that, we can use the result to generate whitelist RDD by Spark's sort and take function, both of which can parallelly compute in the cluster.
Then scSpark can easily use whitelist RDD to filter cached FASTQ R1 RDD and generate extracted FASTQ R1 RDD.
% In the time to filter FASTQ R2 RDD, scSpark uses same strategies again. 
% It uses extracted FASTQ R1 RDD to filter FASTQ R2 RDD when they have same line index.
The last step in sequence quality control was using join function again to combine filtered FASTQ R1 RDD and FASTQ R2 RDD with the same index. 

% (the whitelist of barcodes is obtained by thresholding the number of occurrences)


\iffalse
As shown in Fig~\ref{fig1}, the sequence quality control step typically consists of two main components.
First, generating whitelist RDD including identities of the true cell barcodes is unknown.
After FASTQ R1 RDD generated, we calculated the number of occurences for each CB and selected the most frequent CBs, named whitelist RDD, as intermediate results. 
By using reduceByKey function, each cell barcode could be counted parallelly in each split of FASTQ R1 RDD and we could merge each split's result in the end. 
After that, we can use the result to generate whitelist RDD by Spark's sort and take function, both of which can parallelly compute in the cluster.
Then scSpark can easily uses whitelist RDD to filter cached FASTQ R1 RDD by using Spark's join function to generate extracted FASTQ R1 RDD.
In the time to filter FASTQ R2 RDD, scSpark uses same strategies again. 
It uses extracted FASTQ R1 RDD to filter FASTQ R2 RDD when they have same line index.
The last step in sequence quality control was using join function again to combine filtered FASTQ R1 RDD and FASTQ R2 RDD with the same index. 
\fi

\subsection{Aligner}
\begin{figure}
\centering
	\includegraphics[width=0.35\textwidth]{figure2.png}
	\caption{Using JNI and RDD to integrate the STAR aligner in scSpark.} \label{fig2}
\end{figure}

Intensive disk access consumes large part of process time in align step.
In traditional pipelines, they are not designed for in-memory computing, the sequence aligner loads FASTQ files from harddisks. 
Similar to CellRanger, UMI-tools, and STARsolo, we use STAR as the aligner in scSpark. 
We encapsulated the STAR aligner as a shared object.
Furthermore we re-designed the interface for STAR's load FASTQ files and write SAM files way with Java Native Interface (JNI).
As Fig.~\ref{fig2} shown, we directly transfers FASTQ RDD data to STAR and transfers STAR's result to SAM RDD without any disk access. 
In this way scSpark avoids the heavy burden on disk IO for reading FASTQ files and writing SAM files. 

\iffalse
% Apart from Cellranger we can't know how it implements because it isn't an open source software. 
In previous pipeline, STAR needs to load extracted FASTQ R2 file and FASTQ R1 file.
This way will generate unneccessary disk access. 
We chose STAR as our aligner and modified STAR's loading FASTQ way to avoid loading extracted FASTQ R2 and FASTQ R1 twice. 
As shown in Fig~\ref{fig2}, we utilized Java Native Interface to transfer extracted FASTQ R2 RDD's data to STAR program, and then run the STAR process.
The STAR aligner was encapsulated as a shared object and invoked the shared object as our aligner. 
\fi


% Spark有repartition函数可以直接进行分区，然后我们比如4000000个read(FASTQ R2 RDD)，4个node会根据node情况分成4个区，每个区1000000个reads(FASTQ R2 RDD)
Furthermore, to balance the data processing tasks among different computing nodes, we repartitioned the FASTQ R2 RDD before transfer data to STAR. 
% 可以改成随着单台机器的内存越来越大吗？
In addition, for high-performance workstations with a increasing memory, it is possible to run multiple STAR instances in one node to further speed up the alignment process. 

\iffalse
To overcome imbalance problem of each node's data volume, we repartitioned the extracted FASTQ R2 RDD before transfer FASTQ RDD to STAR.
Each node could run STAR parallelly with counterpoise data volume. 
We also found if node's in-memory was enough, we could start up more than one STAR's in one node to achieve better parallel computing.
After align, STAR's output is SAM file or BAM file that will generate more disk access waste than STAR's input.
Here, We used Java Native Interface again to transfer STAR results and directly abstract them as SAM RDD. 
\fi


\subsection{Count with multi-node}
\begin{figure}
\centering
	\includegraphics[width=0.45\textwidth]{figure3.png}
	\caption{An overview of Spark version count.} \label{fig3}
\end{figure}

The gene transfer format (GTF) files contains gene structure information and are used for counting gene expressions of cells.  
In scSpark, we implemented a multi-node in-memory counting approach. 
First, the GTF files are loaded to memory as GTF RDD objects. 
% 修改成 After that scSpark grouped SAM RDD and GTF RDD by their record's chromosome information.
(After that scSpark grouped SAM RDD and GTF RDD according to their cell name. ???)
Then we used the Spark flatMap function to compute the read count for each cell in parallel. 
\iffalse
As shown in Fig~\ref{fig3}, to take advantage of multi-node's compute capability, we used a new way to implement the count step. 
Except for SAM RDD that were generated in the previous step, we loaded GTF file to memory and abstract them as GTF RDD. 
After that scSpark grouped SAM RDD and GTF RDD according to their cell name. 
Then we used flatMap function to parallelly compute each cell's read count. 
\fi
In the end, we collected results in all nodes, and the result files would produce the only output disk access in the whole program. 
ScSpark breaks the limitation of one node's computing and achieved large scalability. 

\subsection{Experiment design}
We used Apache Spark version 2.1.0 as scSpark's in-memory computing environment.

Three scRNA-seq datasets~(\url{https://support.10xgenomics.com/}) containing approximately 10,000 peripheral blood mononuclear cells (PBMCs) from three different species, namely human, rat and monkey, generated by 10X Genomics platform were used in our experiments. In total, there are 640 million, 289 million and 262 million reads respectively in the raw data of the PBMC\_human dataset, the PBMC\_rat dataset and the PBMC\_monkey dataset.

We evaluated the performance of scSpark with three state\-of\-the\-art pipelines including UMI-tools, CellRanger and STARsolo in terms of the efficiency, scalability and biological analysis accuracy.
We tested the three pipelines on a workstation with 64 CPU cores, and record the time spent for each processing step when the program is assigned with different number of cores. 
\iffalse
Limited by the number of CPU cores of single machine, we only used 64 CPU cores for these three pipelines that are not able to run on computational clusters.
Then, we tested each step's process spend time.
Due to CellRanger isn't an open source software and STAR-solo implemented pipeline in a different way.
And scSpark refers to UMI-tools, we choose UMI-tools each step's performance as scSpark's each step's performance baseline.
\fi


Except performance evaluate, we calculated scSpark's speed improve rate when Spark cluster CPU cores improve.
And we used tradition pipelines' speed improve rate as baseline when pipelines' CPU cores improve from 32 to 64.
After that, we tested scSpark's each step scalability and comapred with UMI\-tools.
In align step, four pipelines' align step all based on STAR's program.
So we tested STAR and scSpark's mapping speed to prove scSpark not only get greatly improve in same CPU cores compare with STAR, but also can get near linear improve when CPU cores increase.

ScSpark is developed based on UMI-tools. 
And UMI-tools accuracy was fully verified. 
This section we used the gene expression matrix that was obtained by scSpark and UMI-tools under the same dataset to perform downstream analysis of scRNA-seq data. 
And then we compared two tools transcript analysis's result and cell cluster analysis's result to verify the correlation between scSpark and UMI-tools. 
Under hgmm-10k-v3 dataset, we used scSpark and UMI-tools to get gene express matrix, compared two tools' result, and computed their correlation.

\section{Results}
%First, we compared each pipeline's speed in same CPU cores number.Due to scSpark refers UMI\-tools, then we compared each step's speed of scSpark with UMI\-tools.After that, we contrasted each pipleline's scalability, and contrasted each step's scalability with UMI\-tools.In the end, we verified scSpark's result by comparing with UMI\-tools's result.

\subsection{Efficiency evaluation}

\iffalse
\begin{table}
	\centering
	\caption{previous pipeline and scSpark's performance comparisione}\label{tab1}
	\resizebox{0.45\textwidth}{!} {
	\begin{tabular}{l | l | l | l }
		\hline
		Pipeline & Dataset & Cores & Spend time(s) \\
		\hline
		UMI-tools & pbmc\_human & 64 cores & 7284 \\
		CellRanger & pbmc\_human & 64 cores & 2225 \\
		STAR-solo & pbmc\_human & 64 cores &  1987 \\
		scSpark & pbmc\_human & 4*16 cores & 355 \\
		\hline
		UMI-tools & pbmc\_rat & 64 cores & 7259 \\
		CellRanger & pbmc\_rat & 64 cores & 1999 \\
		STAR-solo & pbmc\_rat & 64 cores &  1854 \\
		scSpark & pbmc\_rat & 4*16 cores & 326 \\
		\hline
		UMI-tools & pbmc\_monkey & 64 cores & 7809 \\
		CellRanger & pbmc\_monkey & 64 cores & 1891 \\
		STAR-solo & pbmc\_monkey & 64 cores &  2357 \\
		scSpark & pbmc\_monkey & 4*16 cores & 391 \\
		\hline
	\end{tabular}
	}
\end{table}
\fi

\begin{table}
	\centering
	\caption{Comparison of processing time (seconds) of four pipelines.}\label{tab1}
	\begin{tabular}{l | l | l | l }
		\hline
		 & PBMC\_human & PBMC\_rat & PBMC\_monkey \\ 
		\hline
		UMI-tools & 7284 & 7259 & 7809 \\
		CellRanger & 2225 & 1999 & 1891 \\
		STARsolo & 1987 & 1854 & 2357 \\
		scSpark & 355 & 326 & 391 \\
		\hline
	\end{tabular}
\end{table}

We first evaluated the total time comsumption for different pipelines processing the same dataset with the same numbers of CPUs. 
For UMI-tools, CellRanger, and STARsolo, we used all 64 cores of the workstation. 
For scSpark, we used a cluster of 4 computing nodes, where each node has 16 CPU cores. 
Results show that scSpark achieves substantially higher processing speed, which is nearly 20-fold faster than UMI-tools and about 5-fold faster than CellRanger and STARsolo (Table~\ref{tab1}). 
We also recorded the processing time for three individual modules of the pipelines, i.e., barcode extraction and sequence QC, genome alignment, and transcript quantification, to have a more detailed understanding of the performance gain. 
As scSpark is built based on UMI-tools, the breakdown comparison was only between UMI-tools and scSpark, to demonstrate the performance gain brought by distributed and in-memory computing. Compared with UMI-tools, scSpark had significantly shorter processing time in all three steps (Fig.~\ref{fig4}). It is noteworthy that scSpark achieved a processsing speed of approximately 50-fold  faster than UMI\-tools for the barcode extraction and sequence QC step, which turned out to be the dominant factor of the performance promotion brought by scSpark. This result reflects that distributed reading of FASTQ reads from disk and totally removal of re-writing extracted FASTQ reads back to disk that are both performed in scSpark can dramatically reduce the time consumption of scRNA-seq data processing pipelines. 

\iffalse
To illustrate the main source of the significant improvement brought by scSpark, we also compared the efficiency of three individual steps of the pipeline between scSpark and UMI-tools. 
%Table~\ref{tab1} gives a summary of scSpark's performance and compares with tradition pipeline's performance.
%We can see scSpark's speed is much quicker than any tradition pipelines in same CPU cores environment in three datasets.
\fi

\begin{figure*}
	\includegraphics[width=0.8\textwidth]{Fig4.pdf}
	\caption{Comparison of UMI\-tools and scSpark's first step.} \label{fig4}
\end{figure*}

%As shown in Fig.~\ref{fig4}, scSpark achieved significantly shorter processing time in all three steps. 
%For barcode extraction and quality control, scSpark achieved approximate 50x-fold faster than UMI\-tools.
%For sequence alignment, scSpark's get approximate 3x-fold faster than UMI-tools mostly due to reduce disk access.
%For read count, due to each cell record's number imbalance, scSpark's speedup rate isn't stable.
\iffalse
After that we recorded each step's process time to find the reason of scSpark's speed improvement.
As table~\ref{fig4} shown, scSpark is much faster than the UMI-tools in any single step.
Contrast with UMI-tools, scSpark gets great improve in first two steps, cell barcode and UMI correct and quality control.
This step's improve comes from scSpark's in-memory trait which can reduce disk access times.
And scSpark can take advantage of Spark's high concurrency feature when UMI-tools processed record with single thread.

After that, scSpark use in-memory FASTQ RDD and SAM RDD to replace tradition pipeline's disk access. 
Although scSpark and UMI-tools's align step both based on STAR, scSpark can get great improve in same CPU cores condition.
Except reduce redundancy disk access, scSpark processes multi STAR program across Spark cluster in the same time also can improve scSpark's mapping speed.

After align step, we grouped SAM RDD and GTF RDD by cell and concurrency processed each group in each CPU cores.
Due to each cell's record number imbalance, scSpark's count step has a little data skew problem which causes count step's processing time influenced by dataset.
This way also gets great improve compare with UMI-tools.
\fi

\subsection{scalability evaluation} 
Contrast with tradition pipelines, scSpark not only can perform well in same CPU cores but also scSpark can scala out to improve process speed.
\begin{figure*}
	\includegraphics[width=0.8\textwidth]{fig5.pdf}
	\caption{An overview of tradition pipelines' speedup.} \label{fig5}
\end{figure*}
Due to the limited of single machine CPU cores, we tested three tradition pipelines' speedup from 16 CPU cores to 64 CPU cores.
And due to the limited of memory capacity per node, we tested scSpark's speedup starting from 128 (8*16) CPU cores to 256 (16*16) CPU cores.
As shown in Fig~\ref{fig5}, we found that UMI\-tools' speedup is poor when CPU cores number increase.
Except align step, UMI-tools processes on single thread which causes it can't take advantage of CPU cores number increase.
So UMI\-tools' scalability is weak.
STAR-solo and Cellranger shows poorer scalability when CPU cores number increase.
When CPU cores number increased from 16 to 32, STAR-solo achieved approximate average 40\% speedup and Cellranger achieved approximate 10\% speedup.
When CPU cores number increased from 32 to 64, STAR-solo's speedup was average 20\% and Cellranger speedup was average 4.6\%.
Both pipelines speedup rate will decrease when CPU cores increase.

\begin{figure}
	\includegraphics[width=0.45\textwidth]{fig6.pdf}
	\caption{scSpark's speedup.} \label{fig6}
\end{figure}
As shown in Fig~\ref{fig6}, we found it can achieve average 50\% speedup when Spark's CPU cores number increased from 128 to 256.
So scSpark shows more scalability than three tradition pipelines when CPU cores number increase.

Due to UMI\-tools' processes on single thread except align step, this pipelines' scalability most come from align step.
% scSpark不同核心数不同步骤的表现
\begin{table}
	\centering
	\caption{STAR's mapping speed}\label{tab3}
	\resizebox{0.45\textwidth}{!} {
	\begin{tabular}{l | l | l | l | l | l | l}
		\hline
		Dataset & 2 cores & 4 cores & 8 cores & 16 cores & 32 cores & 64 cores \\
		\hline
		PBMC\_human & 120.97 & 242.08 & 449.09 & 772.13 & 1389.32 & 961.59 \\
		PBMC\_rat &  & 261.23 & 530.57 & 1089 & 1391.24 & 1347.35 \\
		PBMC\_monkey  & 38.7 & 76.38 & 170.01 & 325.85 & 512.59 & 828.99 \\
		\hline
	\end{tabular} }
\end{table}
As shown in Table~\ref{tab3}, we found in PBMC\_human and PBMC\_rat STAR can achieved nearly linear improved with CPU cores number increase, when CPU cores number below 32.
STAR's speedup rate not only decrease when CPU cores number increase but also have a bottleneck when CPU cores number increase from 32 to 64.
\begin{figure}
	\includegraphics[width=0.45\textwidth]{fig7.pdf}
	\caption{scSpark's mapping speedup.} \label{fig7}
\end{figure}
As Fig~\ref{fig7} shown, we found scSpark's mapping speed achieved about 79\% parallel efficiency when CPU cores number increase from 128 to 256.
ScSpark enable distributed process align step per node across Spark cluster, so it can get nearly linear parallel efficiency.
So scSpark's align step is more scalability than tradition pipelines' aligner. 

\subsection{Reliability of the results produced by scSpark} 
We further evaluated the reliability of the expression matrices generated by scSpark. 
As Fig~\ref{fig9} shown, for each after processing cell barcode, their gene expression matrix approximate fit $y=x$, $R^{2}$ closed to 0.9998. 
\begin{figure}
  \includegraphics[width=0.45\textwidth]{fig9.pdf}
  \caption{correlation of scSpark and UMI-tools.} \label{fig9}
\end{figure}
Furthermore, we used Seurat to print tSNE picture which based on gene expression matrix generated by ScSpark and UMI-tools. 
And as Fig~\ref{fig10} shown, tSNE cell clustering result showed high corrleation between ScSpark and UMI-tools. 
\begin{figure*}
  \includegraphics[width=\textwidth]{fig10.pdf}
  \caption{tSNE picture based on scSpark and UMI-tools' gene expression matrix.} \label{fig10}
\end{figure*}

\section{Conclusion}
In this paper, we proposed a way which utilize Apache Spark's in-memory compute and distributed compute trait to strengthen upstream scRNA-seq pipeline.
We developed scSpark, which processes more efficiency and more scalable than tradition pipelines.

Refer to UMI-tools, scSpark divided the upstream process into four steps, UMI and CB correct, sequence quality control, genome alignment and transcript counting.
we found scSpark's can get higher performance than three tradition pipelines.
And we found scSpark's each steps can get higher performance than UMI-tools.
We also compared scSpark's result with UMI-tools to verified scSpark's result correct.

\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
